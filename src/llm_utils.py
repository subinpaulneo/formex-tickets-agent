import os
import json
import google.generativeai as genai
from dotenv import load_dotenv
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import re
import outlines # Import outlines
from pydantic import BaseModel # Import BaseModel for defining schema

# Load environment variables from .env file
load_dotenv()

# Configure Google Gemini API
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
if GEMINI_API_KEY:
    genai.configure(api_key=GEMINI_API_KEY)
else:
    print("Warning: GEMINI_API_KEY not found in .env file. LLM functions may not work.")

# Define the model to use for agents (Gemini 2.0 Flash-Lite)
#AGENT_MODEL_NAME = "gemini-1.5-flash-latest" # Using latest flash model
AGENT_MODEL_NAME = "gemini-2.0-flash-lite"
LOCAL_MODEL_ID = "google/gemma-2b-it" # Updated model ID

# Define a custom exception for rate limits or API errors that should trigger a retry
class GeminiAPIError(Exception):
    pass

@retry(
    stop=stop_after_attempt(5), # Try up to 5 times
    wait=wait_exponential(multiplier=1, min=4, max=60), # Exponential backoff: 4s, 8s, 16s, 32s, 60s
    retry=retry_if_exception_type(GeminiAPIError), # Only retry on our custom API error
    reraise=True # Re-raise the last exception if all retries fail
)
def _generate_content_with_retry(model, prompt, generation_config=None):
    try:
        if generation_config:
            response = model.generate_content(prompt, generation_config=generation_config)
        else:
            response = model.generate_content(prompt)
        
        # Check for specific error messages that indicate rate limits or transient issues
        # The Gemini API client might raise an.exception directly, but sometimes errors are in the response text
        if response.text and ("quota" in response.text.lower() or "rate limit" in response.text.lower()):
            raise GeminiAPIError(f"Rate limit or quota error: {response.text}")
        
        return response
    except Exception as e:
        # If it's a specific API error that we want to retry, re-raise as GeminiAPIError
        # For now, we'll catch all exceptions and re-raise as GeminiAPIError to trigger retry
        # In a production system, you'd inspect 'e' more carefully
        raise GeminiAPIError(f"API call failed: {e}") from e

def query_llm(prompt: str, json_mode: bool = False) -> str | dict:
    """
    Sends a prompt to the Google Gemini API and returns the response.

    Args:
        prompt: The question or instruction for the language model.
        json_mode: If True, attempts to parse the LLM's response as JSON.

    Returns:
        The text generated by the model, or a parsed JSON object if json_mode is True, or an error message.
    """
    if not GEMINI_API_KEY:
        return "Error: GEMINI_API_KEY is not set in the .env file."

    model = genai.GenerativeModel(AGENT_MODEL_NAME)

    try:
        # For JSON mode, instruct the model to output JSON
        if json_mode:
            prompt_with_json_hint = prompt + "\n\nRespond with a valid JSON object."
            response = _generate_content_with_retry(
                model,
                prompt_with_json_hint,
                generation_config=genai.GenerationConfig(response_mime_type="application/json")
            )
            # The response.text will already be a JSON string if response_mime_type is set
            generated_text = response.text
            try:
                parsed_json = json.loads(generated_text)
                return parsed_json
            except json.JSONDecodeError:
                print(f"Warning: LLM did not return valid JSON in json_mode. Raw response: {generated_text}")
                return {"error": "LLM did not return valid JSON", "raw_response": generated_text}
        else:
            response = _generate_content_with_retry(model, prompt)
            return response.text

    except GeminiAPIError as e: # Catch our custom error after retries
        # Ensure that in json_mode, an error always returns a dict
        if json_mode:
            return {"error": f"Error querying LLM after retries: {e}", "raw_response": str(e)}
        else:
            return f"Error querying LLM after retries: {e}"
    except Exception as e: # Catch any other unexpected errors
        if json_mode:
            return {"error": f"Unexpected error querying LLM: {e}", "raw_response": str(e)}
        else:
            return f"Unexpected error querying LLM: {e}"

# Define the Pydantic model for the expected JSON output
class TicketInfo(BaseModel):
    category: str
    problem: str
    steps_taken_to_solve: str
    final_solution: str

def query_local_llm(prompt: str) -> str:
    """
    Sends a prompt to the local Gemma model and returns the response.

    Args:
        prompt: The question or instruction for the language model.

    Returns:
        The text generated by the model.
    """
    device = "cuda" if torch.cuda.is_available() else "mps" if torch.backends.mps.is_available() else "cpu"
    print(f"Using device: {device}")

    # Reverted to AutoTokenizer, AutoModelForCausalLM
    tokenizer = AutoTokenizer.from_pretrained(LOCAL_MODEL_ID)
    model = AutoModelForCausalLM.from_pretrained(
        LOCAL_MODEL_ID,
        torch_dtype=torch.bfloat16,
    ).to(device)

    # Use outlines to generate structured output
    # The prompt should be the full prompt including the JSON instructions
    # Outlines will ensure the output adheres to the TicketInfo schema
    o_model = outlines.from_transformers(model, tokenizer)
    
    # The prompt is now passed directly to the outlines model call
    generated_json_string = o_model(prompt, TicketInfo, max_new_tokens=4096)

    # Outlines returns a JSON string, validate it into a Pydantic object
    # and then convert it back to JSON string for consistency with query_llm output
    ticket_info_instance = TicketInfo.model_validate_json(generated_json_string)
    return ticket_info_instance.model_dump_json()


if __name__ == '__main__':
    # This allows for testing the LLM connection directly
    print("Testing LLM connection...")
    test_prompt = "Explain what a language model is in one sentence."
    print(f"Prompt: {test_prompt}")
    response = query_llm(test_prompt)
    print(f"Response: {response}")

    print("\nTesting LLM connection with JSON mode...")
    test_json_prompt = "Provide a JSON object with your name and age."
    print(f"Prompt: {test_json_prompt}")
    json_response = query_llm(test_json_prompt, json_mode=True)
    print(f"Response: {json_response}")

    print("\nTesting local LLM with structured output...")
    test_local_json_prompt = "Extract category, problem, steps_taken_to_solve, final_solution from: Customer complains about slow shipping. Agent provided tracking number. Issue resolved."
    print(f"Prompt: {test_local_json_prompt}")
    local_json_response = query_local_llm(test_local_json_prompt)
    print(f"Response: {local_json_response}")
